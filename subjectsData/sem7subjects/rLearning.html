<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Experiments List</title>
    <link rel="stylesheet" href="../../styles.css">
    <style>
       body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            color: #333;
            background-color: #f5f5f5;
        }

        .navbar {
            background-color: #0B0425;
            color: #fff;
            padding: 15px 20px;
        }

        .navbar .brand {
            text-decoration: none;
            color: #fff;
            font-size: 24px;
            font-weight: bold;
        }

        .semester1 .section {
            background-color: #1f2937;
            padding: 4rem 2rem;
            margin-top: 4rem;
            position: relative;
            max-width: 1200px;
            margin-left: auto;
            margin-right: auto;
            text-align: center;
        }

        .semester1 .section-title {
            font-size: 2.5rem;
            margin-bottom: 1.5rem;
            color: #f3f4f6;
            font-weight: 800;
        }

        .semester1 .practicals-list {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }

        .semester1 .practicals-list .card {
            background: linear-gradient(to right, rgb(45, 45, 197),rgba(64, 201, 116, 0.678), rgb(45, 45, 197));
            border-radius: 0.375rem;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            color: #f3f4f6;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            transition: background-color 0.3s ease, transform 0.3s ease;
            text-align: center;
        }

        .semester1 .practicals-list .card:hover {
            background-color: #4b5563;
            transform: scale(1.02);
        }

        .semester1 .card-title {
            font-size: 2rem;
            margin-bottom: 1rem;
            color: #ffffff;
            font-weight: 700;
        }

        .semester1 .theoretical-content,
        .semester1 .code-block {
            display: none;
            text-align: left;
            overflow: hidden;
            transition: max-height 0.4s ease;
            background-color: #1e293b;
            padding: 20px;
            border-radius: 0.375rem;
            margin-top: 1rem;
        }

        .semester1 .theoretical-content h3,
        .semester1 .theoretical-content h4,
        .semester1 .theoretical-content h5,
        .semester1 .theoretical-content p,
        .semester1 .theoretical-content ul {
            text-align: left;
            color: #f3f4f6;
        }

        .semester1 .theoretical-content h3 {
            font-size: 2rem;
            margin-bottom: 1rem;
        }

        .semester1 .theoretical-content h4 {
            font-size: 1.75rem;
            margin-top: 1.5rem;
        }

        .semester1 .theoretical-content h5 {
            font-size: 1.5rem;
            margin-top: 1rem;
        }

        .semester1 .theoretical-content p {
            font-size: 1.125rem;
            margin: 0.5rem 0;
            line-height: 1.8;
        }

        .semester1 .theoretical-content ul {
            margin: 0.5rem 0;
            padding-left: 1.5rem;
        }

        .semester1 .theoretical-content ul li {
            margin-bottom: 0.5rem;
        }

        .semester1 .code-block {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 1rem;
            border-radius: 0.375rem;
            overflow-x: auto;
            font-family: 'Courier New', Courier, monospace;
            white-space: pre-wrap;
        }

        .reveal-button {
            background-color: #0B0425;
            color: #fff;
            border: none;
            padding: 0.75rem 1.5rem;
            border-radius: 0.375rem;
            cursor: pointer;
            font-size: 1.125rem;
            margin-top: 1rem;
            transition: background-color 0.3s ease;
            display: block;
            width: 100%;
            text-align: center;
        }

        .reveal-button:hover {
            background-color: rgb(47, 130, 255);
        }

        .container {
            display: inline-block;
            justify-content: space-between;
            align-items: flex-start;
            gap: 2rem;
            width: 200px;
            margin-left: auto;
            margin-right: auto;

        }

        .semester1.section {
            flex: 1;
        }

        .video-section {
            
            background-color: #1f2937;
            padding: 2rem;
            border-radius: 0.375rem;
            text-align:left;
        }
        .video-section2 {
          
            background-color: #1f2937;
            padding: 2rem;
            border-radius: 0.375rem;
            text-align:right;
        }

        .video-title {
            font-size: 2rem;
            margin-bottom: 1rem;
            color: #f3f4f6;
            font-weight: 700;
        }

        .video-container {
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .video-container iframe {
            border-radius: 0.375rem;
        }
        .download-button {
            width: 30vw;
            display: inline-block;
            padding: 1vh 2vh;
            font-size: 3vh;
            color: #fff;
            background-color: #007bff;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            text-decoration: none;
            margin-bottom: 3vh;
        }

        .download-button:hover {
            background-color: #0056b3;
        }

        .INFO
        {
            padding: 1vh 2vh;
            font-size: 4vh;
            display: block;
            margin-bottom: 2vh;
            color: rgb(255, 255, 255);
            align-items: center;
            justify-content: center;
            background-color: #0056b3;
            border-radius: 10px;
            max-width: 50vw;
            max-height: 20vw;
            margin: 4vh auto;
        }

        pre1 .output
        {
            display: inline-block;
            justify-content: space-between;
        }
               
@media (max-width: 425px) {
    .semester1 .theoretical-content h3 {
        font-size: 1.6rem;
        margin-bottom: 1rem;
    }

    .semester1 .theoretical-content h4 {
        font-size: 1.35rem;
        margin-top: 1.5rem;
    }

    .semester1 .theoretical-content h5 {
        font-size: 1.3rem;
        margin-top: 1rem;
    }

    .semester1 .theoretical-content p {
        font-size: 1rem;
        margin: 0.5rem 0;
        line-height: 1.8;
    }

    .semester1 .theoretical-content ul {
        margin: 0.3rem 0;
        padding-left: 1.5rem;
    }

    .semester1 .theoretical-content ul li {
        margin-bottom: 0.3rem;
    }

   .video-section iframe{
        width: 230px;
        height: 157px;
    }
    .card{
        width: 95%;
    }

    .semester1 .theoretical-content,
        .semester1 .code-block {
            display: none;
            text-align: left;
            overflow: hidden;
            transition: max-height 0.4s ease;
            background-color: #1e293b;
            padding: 15px;
            border-radius: 0.375rem;
            margin-top: 1rem;
        }
        .Output img{
            width: 260px;
            height: 170px;
        }
        .code-block code{
                font-size: 0.8rem;
        }
    
}
    </style>
</head>

<body>
    <!-- Navbar -->
    <nav class="navbar">
        <div class="container">
            <a href="../../semestershtml/semester7.html" class="brand">Back to Subjects</a>
        </div>
    </nav>

    <!-- Experiments Section -->
    <section class="semester1 section">
        <h1 class="section-title">Pattern Recognition and Computer Vision  </h1>

    <a href="files/sem7.pdf" download="ALL.pdf" class="download-button">Download PDF</a>
    <div class="INFO">

        Video + Theory 
        
    </div>

        <ul class="practicals-list">

            <li class="card">
                <h2 class="card-title">1. Setting up the Spyder IDE Environment and Executing a Python Program</h2>
                <button class="reveal-button" onclick="toggleContent('theoryContent1')">Toggle Content</button>
                <div id="theoryContent1" class="theoretical-content">
                    <h3>Overview of Spyder IDE</h3>
                    <p>Spyder is an open-source Integrated Development Environment (IDE) for Python, specifically designed for scientific programming and data analysis. It provides a user-friendly interface with features such as an interactive console, variable explorer, and editor for writing and debugging code.</p>
                    
                    <h3>Key Concepts</h3>
                    <h4>1. Installation</h4>
                    <p><strong>Definition:</strong> Installing Spyder IDE involves downloading the software and setting it up on your computer. This can be done using package managers like Anaconda or through direct installation from the Spyder website.</p>
                    <p><strong>Example:</strong> To install Spyder using Anaconda, run the command <code>conda install spyder</code> in your terminal or command prompt.</p>
                    
                    <h4>2. Environment Setup</h4>
                    <p><strong>Definition:</strong> Configuring the IDE environment involves setting up your workspace, selecting the Python interpreter, and installing necessary libraries for your projects.</p>
                    <p><strong>Example:</strong> Set the Python interpreter in Spyder by going to <strong>Tools</strong> -> <strong>Preferences</strong> -> <strong>Python Interpreter</strong> and selecting the desired interpreter.</p>
                    
                    <h4>3. Executing a Python Program</h4>
                    <p><strong>Definition:</strong> Running a Python program in Spyder involves writing the code in the editor and executing it using the Run button or shortcut keys.</p>
                    <p><strong>Example:</strong> Write a simple Python script like <code>print("Hello, World!")</code> and execute it by pressing <code>F5</code> or clicking the Run button.</p>
                    
                    <h3>Practical Use of Spyder IDE</h3>
                    <h4>Installing Spyder</h4>
                    <p>Download Spyder from the official website or install it using a package manager. Follow the installation instructions to set it up on your machine.</p>
                    
                    <h4>Setting Up Your Environment</h4>
                    <p>Open Spyder and configure your workspace. Set the Python interpreter and install any additional libraries required for your projects using the terminal within Spyder or Anaconda Navigator.</p>
                    
                    <h4>Writing and Running a Python Program</h4>
                    <p>Open a new file in the Spyder editor, write your Python code, and execute it to see the results in the console.</p>
                    

                    <h3>Viva Questions and Answers</h3>
                    <p>1. What is Spyder IDE?</p>
                    <p><strong>Answer:</strong> Spyder IDE is an open-source Integrated Development Environment for Python, tailored for scientific programming and data analysis, offering features like an interactive console, variable explorer, and code editor.</p>
            
                    <p>2. How do you install Spyder IDE?</p>
                    <p><strong>Answer:</strong> Spyder can be installed through Anaconda by running <code>conda install spyder</code> or directly from the Spyder website by downloading the installer for your operating system.</p>
            
                    <p>3. How do you set up the environment in Spyder?</p>
                    <p><strong>Answer:</strong> To set up the environment, open Spyder, go to <strong>Tools</strong> -> <strong>Preferences</strong> -> <strong>Python Interpreter</strong> and select the desired Python interpreter. You can also install additional libraries through the terminal or Anaconda Navigator.</p>
            
                    <p>4. How do you execute a Python program in Spyder?</p>
                    <p><strong>Answer:</strong> Write your Python code in the Spyder editor and execute it by pressing <code>F5</code> or by clicking the Run button in the toolbar. The output will be displayed in the console pane.</p>


                    <!-- Embedded YouTube Video -->
                    <div id="videoBlock1" class="video-section">
                        <h3>How to Set Up Spyder IDE and Execute a Python Program</h3>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/JyPjRrF5MXE" 
                                title="YouTube video player" frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                allowfullscreen></iframe>
                    </div>
            
                    <h3>Sample Python Code</h3>
                    <pre><code>
            # Sample Python Program for Setting up the Spyder IDE Environment and Executing a Python Program
            print("Hello, World!")
                    </code></pre>
                     
                    <div class="Apparatus & Instruments">
                        <!-- Add relevant information here -->
                    </div>
                </div>
            
               
                </div>
            </li>
            
            <li class="card">
                <h2 class="card-title">2. Installing Keras, TensorFlow, and PyTorch Libraries and Making Use of Them</h2>
                <button class="reveal-button" onclick="toggleContent('theoryContent2')">Toggle Content</button>
                <div id="theoryContent2" class="theoretical-content">
                    <h3>Overview of Libraries</h3>
                    <p>Keras, TensorFlow, and PyTorch are popular libraries for deep learning. Keras provides a high-level API for building and training deep learning models, TensorFlow is an end-to-end open-source platform for machine learning, and PyTorch is known for its dynamic computation graph and strong support for research.</p>
                    
                    <h3>Key Concepts</h3>
                    <h4>1. Installation</h4>
                    <p><strong>Definition:</strong> Installing these libraries involves using package managers like pip to download and set up the libraries in your Python environment.</p>
                    <p><strong>Example:</strong> Install Keras, TensorFlow, and PyTorch using the following commands:
                    <pre><code>
pip install keras
pip install tensorflow
pip install torch
                    </code></pre>
                    </p>
                    
                    <h4>2. Using the Libraries</h4>
                    <p><strong>Definition:</strong> After installation, you can import and use these libraries to build and train deep learning models.</p>
                    <p><strong>Example:</strong> Import and use Keras to build a simple neural network model:
                    <pre><code>
from keras.models import Sequential
from keras.layers import Dense
            
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(10,)))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
                    </code></pre>
                    </p>
                    
                    <h3>Practical Use</h3>
                    <h4>Installing Libraries</h4>
                    <p>Use pip to install Keras, TensorFlow, and PyTorch. Ensure you have a compatible version of Python and dependencies installed.</p>
                    
                    <h4>Building and Training Models</h4>
                    <p>Use Keras, TensorFlow, or PyTorch to build and train machine learning models. Refer to the documentation of each library for detailed usage and examples.</p>
                    
                    <h3>Viva Questions and Answers</h3>
                    <p>1. What are Keras, TensorFlow, and PyTorch?</p>
                    <p><strong>Answer:</strong> Keras is a high-level API for building and training deep learning models, TensorFlow is an end-to-end open-source platform for machine learning, and PyTorch is known for its dynamic computation graph and strong support for research.</p>
                    
                    <p>2. How do you install Keras, TensorFlow, and PyTorch?</p>
                    <p><strong>Answer:</strong> You can install these libraries using pip. The commands are <code>pip install keras</code>, <code>pip install tensorflow</code>, and <code>pip install torch</code>.</p>
                    
                    <p>3. How do you use Keras to build a neural network model?</p>
                    <p><strong>Answer:</strong> Import Keras and use the <code>Sequential</code> model to build a neural network. Add layers using the <code>add()</code> method and compile the model using <code>compile()</code> with appropriate optimizer and loss function.</p>
                    
                    <p>4. What is the difference between TensorFlow and PyTorch?</p>
                    <p><strong>Answer:</strong> TensorFlow uses a static computation graph while PyTorch uses a dynamic computation graph, making PyTorch more flexible and easier to debug. TensorFlow provides more tools for production and deployment, while PyTorch is favored for research.</p>
                    
                    <p>5. How do you define and use a custom neural network in PyTorch?</p>
                    <strong>Answer:</strong> Define a custom neural network by subclassing <code>nn.Module</code> and implementing the <code>forward()</code> method. Use <code>torch.optim</code> to define the optimizer and <code>nn.BCELoss()</code> for binary classification tasks.</p>
                    <!-- Embedded YouTube Video -->
                    <div id="videoBlock2" class="video-section">
                        <h3>How to Install and Use Keras, TensorFlow, and PyTorch</h3>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/VH4pK4V0_ks" 
                                title="YouTube video player" frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                allowfullscreen></iframe>
                    </div>
            
                    <h3>Sample Code</h3>
                    <pre><code>
# Sample Code for Using Keras
from keras.models import Sequential
from keras.layers import Dense
            
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(10,)))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
            
# Sample Code for Using TensorFlow
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
            
model_tf = Sequential()
model_tf.add(Dense(64, activation='relu', input_shape=(10,)))
model_tf.add(Dense(1, activation='sigmoid'))
model_tf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
            
# Sample Code for Using PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
            
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 64)
        self.fc2 = nn.Linear(64, 1)
        self.sigmoid = nn.Sigmoid()
            
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x
            
model_pt = SimpleNN()
criterion = nn.BCELoss()
optimizer = optim.Adam(model_pt.parameters(), lr=0.001)
                    </code></pre>
                    
                    <div class="Apparatus & Instruments">
                        <!-- Add relevant information here -->
                    </div>
            
                   
                </div>
            </li>            


            <li class="card">
                <h2 class="card-title">3. Implement Q-learning with Pure Python to Play a Game</h2>
                <button class="reveal-button" onclick="toggleContent('theoryContent3')">Toggle Content</button>
                <div id="theoryContent3" class="theoretical-content">
                    <h3>Overview of Q-learning</h3>
                    <p>Q-learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for a given finite Markov decision process. It helps an agent learn how to act optimally in a given environment by learning the value of actions through exploration and exploitation.</p>
                    
                    <h3>Key Concepts</h3>
                    <h4>1. Environment Setup and Introduction to OpenAI Gym</h4>
                    <p><strong>Definition:</strong> OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It provides a variety of environments to test and train agents.</p>
                    <p><strong>Example:</strong> Install OpenAI Gym using the command <code>pip install gym</code> and use it to set up an environment like CartPole.</p>
                    
                    <h4>2. Writing Q-learning Algorithm</h4>
                    <p><strong>Definition:</strong> Q-learning involves creating a Q-table to store values of state-action pairs and updating this table based on the agent's experience and rewards received from the environment.</p>
                    <p><strong>Example:</strong> Implement the Q-learning algorithm using Python as follows:
                    <pre><code>
            import numpy as np
            import gym
            
            # Initialize the environment
            env = gym.make('CartPole-v1')
            n_actions = env.action_space.n
            n_states = np.prod(env.observation_space.shape)
            
            # Initialize Q-table
            Q = np.zeros((n_states, n_actions))
            
            # Parameters
            alpha = 0.1  # Learning rate
            gamma = 0.99 # Discount factor
            epsilon = 0.1 # Exploration rate
            
            # Training loop
            for episode in range(1000):
                state = env.reset()
                done = False
                
                while not done:
                    if np.random.rand() < epsilon:
                        action = env.action_space.sample()
                    else:
                        action = np.argmax(Q[state])
                    
                    next_state, reward, done, _ = env.step(action)
                    
                    # Update Q-value
                    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
                    
                    state = next_state
                    if done:
                        break
                    </code></pre>
                    </p>
                    
                    <h4>3. Training the Agent</h4>
                    <p><strong>Definition:</strong> Train the agent using the Q-learning algorithm by running episodes and updating the Q-table based on the rewards and transitions observed.</p>
                    <p><strong>Example:</strong> Train the Q-learning agent to play a game like CartPole by running multiple episodes and updating the Q-values accordingly.</p>
                    
                    <h4>4. Watching the Trained Agent Play</h4>
                    <p><strong>Definition:</strong> After training, you can visualize the trained agent interacting with the environment to see how well it performs.</p>
                    <p><strong>Example:</strong> Use OpenAI Gym’s rendering capabilities to watch the trained agent play the CartPole game.</p>
                    
                    <h3>Practical Use of Q-learning</h3>
                    <h4>Setting Up the Environment</h4>
                    <p>Install OpenAI Gym and create an environment for training. Use this environment to simulate the game and interact with it.</p>
                    
                    <h4>Implementing and Training Q-learning</h4>
                    <p>Write the Q-learning algorithm, train the agent, and observe how the agent learns to play the game over time.</p>
                    
                    <h4>Visualizing the Agent's Performance</h4>
                    <p>Use visualization tools to watch the trained agent in action and evaluate its performance.</p>
                    

                    
                <div id="vivaQuestions3" class="viva-questions">
                    <h3>Viva Questions and Answers</h3>
                    <p>1. What is Q-learning?</p>
                    <p><strong>Answer:</strong> Q-learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for a given finite Markov decision process. It helps an agent learn how to act optimally by learning the value of actions through exploration and exploitation.</p>
            
                    <p>2. How do you set up the environment for Q-learning?</p>
                    <p><strong>Answer:</strong> Set up the environment using OpenAI Gym by installing it with <code>pip install gym</code> and creating an environment like CartPole using <code>gym.make('CartPole-v1')</code>.</p>
            
                    <p>3. How does the Q-learning algorithm work?</p>
                    <p><strong>Answer:</strong> The Q-learning algorithm involves initializing a Q-table to store values of state-action pairs. The table is updated based on the agent's interactions with the environment and rewards received. The agent explores actions, updates Q-values using the Bellman equation, and learns an optimal policy over time.</p>
            
                    <p>4. How do you train an agent using Q-learning?</p>
                    <p><strong>Answer:</strong> Train the agent by running episodes where the agent interacts with the environment, takes actions based on exploration or exploitation, receives rewards, and updates the Q-values. Continue training over many episodes to improve the agent's performance.</p>
            
                    <p>5. How can you visualize the performance of a trained Q-learning agent?</p>
                    <p><strong>Answer:</strong> Use OpenAI Gym’s rendering capabilities to visualize the agent playing the game. You can call <code>env.render()</code> in the training loop or after training to observe the agent’s actions and performance.</p>
                </div>

                    <!-- Embedded YouTube Video -->
                    <div id="videoBlock3" class="video-section">
                        <h3>How to Implement and Train Q-learning with Pure Python</h3>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/5P39KD0OH0c" 
                                title="YouTube video player" frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                allowfullscreen></iframe>
                    </div>
            
                    <h3>Sample Python Code</h3>
                    <pre><code>
            # Sample Q-learning Implementation with OpenAI Gym
            import numpy as np
            import gym
            
            # Initialize the environment
            env = gym.make('CartPole-v1')
            n_actions = env.action_space.n
            n_states = np.prod(env.observation_space.shape)
            
            # Initialize Q-table
            Q = np.zeros((n_states, n_actions))
            
            # Parameters
            alpha = 0.1  # Learning rate
            gamma = 0.99 # Discount factor
            epsilon = 0.1 # Exploration rate
            
            # Training loop
            for episode in range(1000):
                state = env.reset()
                done = False
                
                while not done:
                    if np.random.rand() < epsilon:
                        action = env.action_space.sample()
                    else:
                        action = np.argmax(Q[state])
                    
                    next_state, reward, done, _ = env.step(action)
                    
                    # Update Q-value
                    Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])
                    
                    state = next_state
                    if done:
                        break
                    </code></pre>
                    
                   
                </div>
            
            </li>
            
            <li class="card">
                <h2 class="card-title">4. Implement Deep Q-Network with PyTorch</h2>
                <button class="reveal-button" onclick="toggleContent('theoryContent4')">Toggle Content</button>
                <div id="theoryContent4" class="theoretical-content">
                    <h3>Overview of Deep Q-Networks (DQN)</h3>
                    <p>Deep Q-Network (DQN) is an extension of Q-learning that leverages deep learning techniques to approximate the Q-value function. This approach allows the handling of large state spaces by using neural networks to estimate Q-values, making it possible to apply Q-learning to complex environments with high-dimensional inputs.</p>
                    
                    <h3>Key Concepts</h3>
                    <h4>1. Environment Setup and Introduction to PyTorch</h4>
                    <p><strong>Definition:</strong> PyTorch is an open-source machine learning library based on the Torch library. It provides tools for building and training deep learning models, including neural networks.</p>
                    <p><strong>Example:</strong> Install PyTorch using the command <code>pip install torch</code> and set up an environment similar to CartPole for training a DQN agent.</p>
                    
                    <h4>2. Implementing Deep Q-Network</h4>
                    <p><strong>Definition:</strong> Implementing a DQN involves creating a neural network to approximate the Q-values, training the network using experience replay and target networks, and applying the DQN algorithm to the chosen environment.</p>
                    <p><strong>Example:</strong> Implement the DQN algorithm using PyTorch as follows:
                    <pre><code>
            import numpy as np
            import torch
            import torch.nn as nn
            import torch.optim as optim
            import gym
            from collections import deque
            import random
            
            # Define the Q-network
            class QNetwork(nn.Module):
                def __init__(self, state_dim, action_dim):
                    super(QNetwork, self).__init__()
                    self.fc1 = nn.Linear(state_dim, 64)
                    self.fc2 = nn.Linear(64, 64)
                    self.fc3 = nn.Linear(64, action_dim)
            
                def forward(self, x):
                    x = torch.relu(self.fc1(x))
                    x = torch.relu(self.fc2(x))
                    x = self.fc3(x)
                    return x
            
            # Initialize the environment and network
            env = gym.make('CartPole-v1')
            state_dim = env.observation_space.shape[0]
            action_dim = env.action_space.n
            policy_net = QNetwork(state_dim, action_dim)
            target_net = QNetwork(state_dim, action_dim)
            target_net.load_state_dict(policy_net.state_dict())
            optimizer = optim.Adam(policy_net.parameters())
            criterion = nn.MSELoss()
            
            # Hyperparameters
            gamma = 0.99
            epsilon = 0.1
            batch_size = 64
            replay_buffer = deque(maxlen=10000)
            
            # Training loop
            for episode in range(1000):
                state = env.reset()
                state = torch.tensor(state, dtype=torch.float32)
                done = False
                
                while not done:
                    if np.random.rand() < epsilon:
                        action = env.action_space.sample()
                    else:
                        with torch.no_grad():
                            q_values = policy_net(state)
                            action = q_values.argmax().item()
                    
                    next_state, reward, done, _ = env.step(action)
                    next_state = torch.tensor(next_state, dtype=torch.float32)
                    replay_buffer.append((state, action, reward, next_state, done))
                    
                    if len(replay_buffer) > batch_size:
                        batch = random.sample(replay_buffer, batch_size)
                        states, actions, rewards, next_states, dones = zip(*batch)
                        states = torch.stack(states)
                        actions = torch.tensor(actions)
                        rewards = torch.tensor(rewards)
                        next_states = torch.stack(next_states)
                        dones = torch.tensor(dones)
                        
                        current_q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()
                        next_q_values = target_net(next_states).max(1)[0]
                        target_q_values = rewards + (1 - dones) * gamma * next_q_values
                        
                        loss = criterion(current_q_values, target_q_values)
                        optimizer.zero_grad()
                        loss.backward()
                        optimizer.step()
                    
                    state = next_state
                    if done:
                        break
            
                # Update the target network
                if episode % 10 == 0:
                    target_net.load_state_dict(policy_net.state_dict())
                    </code></pre>
                    </p>
                    
                    <h4>3. Training the DQN Agent</h4>
                    <p><strong>Definition:</strong> Train the DQN agent by running episodes where the agent interacts with the environment, collects experiences, updates the Q-network using experience replay, and periodically updates the target network.</p>
                    <p><strong>Example:</strong> Run multiple episodes to train the DQN agent on the CartPole environment, adjusting hyperparameters as needed to improve performance.</p>
                    
                    <h4>4. Watching the Trained Agent Play</h4>
                    <p><strong>Definition:</strong> After training, visualize the trained DQN agent interacting with the environment to evaluate its performance.</p>
                    <p><strong>Example:</strong> Use PyTorch and OpenAI Gym’s rendering capabilities to watch the trained DQN agent play the CartPole game.</p>
                    
                    <h3>Practical Use of DQN</h3>
                    <h4>Setting Up the Environment</h4>
                    <p>Install PyTorch and OpenAI Gym. Create an environment for training and set up a Q-network for approximating Q-values.</p>
                    
                    <h4>Implementing and Training DQN</h4>
                    <p>Build the DQN model using PyTorch, train the agent using experience replay, and periodically update the target network.</p>
                    
                    <h4>Visualizing the Agent's Performance</h4>
                    <p>Use visualization tools to watch the trained agent in action and assess its performance.</p>
                    
                    <div id="vivaQuestions4" class="viva-questions">
                        <h3>Viva Questions and Answers</h3>
                        <p>1. What is a Deep Q-Network (DQN)?</p>
                        <p><strong>Answer:</strong> A Deep Q-Network (DQN) is an extension of Q-learning that uses a deep neural network to approximate the Q-value function. It allows for handling complex environments with high-dimensional state spaces by learning a policy through experience replay and target networks.</p>
                
                        <p>2. How do you set up the environment for training a DQN?</p>
                        <p><strong>Answer:</strong> Set up the environment using OpenAI Gym by installing it with <code>pip install gym</code> and creating an environment like CartPole. Initialize PyTorch and build a neural network model for approximating Q-values.</p>
                
                        <p>3. How does the DQN algorithm work?</p>
                        <p><strong>Answer:</strong> The DQN algorithm uses a neural network to approximate the Q-values. It trains the network using experience replay, where experiences are stored in a replay buffer and sampled for training. A target network is used to stabilize training by providing consistent Q-value targets.</p>
                
                        <p>4. How do you train a DQN agent?</p>
                        <p><strong>Answer:</strong> Train the DQN agent by running episodes where the agent interacts with the environment and collects experiences. Use these experiences to update the Q-network through backpropagation, applying experience replay and periodically updating the target network.</p>
                
                        <p>5. How can you visualize the performance of a trained DQN agent?</p>
                        <p><strong>Answer:</strong> Use OpenAI Gym’s rendering capabilities to visualize the agent playing the game. You can call <code>env.render()</code> to observe the agent’s actions and evaluate its performance.</p>
                    </div>
                    
                    <!-- Embedded YouTube Video -->
                    <div id="videoBlock4" class="video-section">
                        <h3>How to Implement and Train Deep Q-Network with PyTorch</h3>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/3j35C5pQwrg" 
                                title="YouTube video player" frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                allowfullscreen></iframe>
                    </div>
                </div>
            </li>
            
            <li class="card">
                <h2 class="card-title">5. Python Implementation of Iterative Policy Evaluation and Update</h2>
                <button class="reveal-button" onclick="toggleContent('theoryContent5')">Toggle Content</button>
                <div id="theoryContent5" class="theoretical-content">
                    <h3>Overview of Iterative Policy Evaluation and Update</h3>
                    <p>Iterative Policy Evaluation and Update is a fundamental concept in Reinforcement Learning and Dynamic Programming. It involves iteratively improving a policy by evaluating and updating it until convergence. The goal is to find an optimal policy that maximizes the expected cumulative reward in a given environment.</p>
                    
                    <h3>Key Concepts</h3>
                    <h4>1. Policy Evaluation</h4>
                    <p><strong>Definition:</strong> Policy evaluation computes the value function for a given policy. It involves iteratively updating the value function until it converges, reflecting the expected return of each state under the policy.</p>
                    <p><strong>Example:</strong> Implement policy evaluation in Python by initializing the value function and iteratively updating it based on the Bellman equation.</p>
                    
                    <h4>2. Policy Improvement</h4>
                    <p><strong>Definition:</strong> Policy improvement involves updating the policy based on the current value function to make it greedy with respect to the value function. This process is repeated until the policy converges to the optimal policy.</p>
                    <p><strong>Example:</strong> Improve the policy by selecting actions that maximize the expected return as indicated by the updated value function.</p>
                    
                    <h4>3. Implementing Iterative Policy Evaluation and Update</h4>
                    <p><strong>Definition:</strong> Combine policy evaluation and policy improvement steps in an iterative process to converge to the optimal policy and value function.</p>
                    <p><strong>Example:</strong> Implement the iterative policy evaluation and update algorithm using Python as follows:
                    <pre><code>
            import numpy as np
            
            # Define the environment
            n_states = 4  # Example state space size
            n_actions = 2  # Example action space size
            gamma = 0.9  # Discount factor
            
            # Initialize policy and value function
            policy = np.ones((n_states, n_actions)) / n_actions
            V = np.zeros(n_states)
            
            # Define the transition probability and reward functions
            def transition_prob(state, action):
                # Example transition probabilities and rewards
                return 0.8, -1  # P(s', r)
            
            def reward(state, action):
                return -1  # Example reward
            
            # Policy Evaluation
            def policy_evaluation(policy, V, gamma):
                theta = 1e-6
                while True:
                    delta = 0
                    for s in range(n_states):
                        v = V[s]
                        new_value = sum(policy[s, a] * (reward(s, a) + gamma * sum(transition_prob(s, a)[0] * V[s_]
                                                    for s_ in range(n_states)))
                                        for a in range(n_actions))
                        V[s] = new_value
                        delta = max(delta, abs(v - V[s]))
                    if delta < theta:
                        break
            
            # Policy Improvement
            def policy_improvement(V, policy, gamma):
                policy_stable = True
                for s in range(n_states):
                    old_action = np.argmax(policy[s])
                    new_action = np.argmax([sum(transition_prob(s, a)[0] * (reward(s, a) + gamma * V[s_])
                                                for s_ in range(n_states)) for a in range(n_actions)])
                    if old_action != new_action:
                        policy_stable = False
                    policy[s] = np.eye(n_actions)[new_action]
                return policy_stable
            
            # Iterative Policy Evaluation and Update
            def policy_iteration(policy, V, gamma):
                while True:
                    policy_evaluation(policy, V, gamma)
                    policy_stable = policy_improvement(V, policy, gamma)
                    if policy_stable:
                        break
            
            # Run Policy Iteration
            policy_iteration(policy, V, gamma)
            print("Optimal Policy:\n", policy)
            print("Optimal Value Function:\n", V)
                    </code></pre>
                    </p>
                    
                    <h4>4. Visualizing the Policy and Value Function</h4>
                    <p><strong>Definition:</strong> After computing the optimal policy and value function, visualize them to understand the learned policy and value function across states.</p>
                    <p><strong>Example:</strong> Use plotting libraries like Matplotlib to visualize the optimal policy and value function.</p>
                    
                    <h3>Practical Use of Iterative Policy Evaluation and Update</h3>
                    <h4>Setting Up the Environment</h4>
                    <p>Define the state and action spaces, transition probabilities, and rewards. Initialize the policy and value function.</p>
                    
                    <h4>Implementing Policy Iteration</h4>
                    <p>Write the policy evaluation and policy improvement functions. Combine them in an iterative process to find the optimal policy and value function.</p>
                    
                    <h4>Visualizing Results</h4>
                    <p>Use visualization tools to display the optimal policy and value function to analyze the results.</p>
                    
                    <div id="vivaQuestions5" class="viva-questions">
                        <h3>Viva Questions and Answers</h3>
                        <p>1. What is Iterative Policy Evaluation and Update?</p>
                        <p><strong>Answer:</strong> Iterative Policy Evaluation and Update is a method in reinforcement learning and dynamic programming for finding the optimal policy by iteratively evaluating and improving the policy until convergence. It involves computing the value function for a given policy and updating the policy to be greedy with respect to the value function.</p>
                
                        <p>2. How does policy evaluation work?</p>
                        <p><strong>Answer:</strong> Policy evaluation involves calculating the value function for a given policy by iteratively updating the value of each state based on the expected returns from the policy. This is done until the value function converges to a stable value.</p>
                
                        <p>3. What is the purpose of policy improvement?</p>
                        <p><strong>Answer:</strong> Policy improvement updates the policy to be greedy with respect to the current value function. The goal is to select actions that maximize the expected return, thus improving the policy based on the value function.</p>
                
                        <p>4. How do you implement iterative policy evaluation and update in Python?</p>
                        <p><strong>Answer:</strong> Implement iterative policy evaluation and update by defining the state and action spaces, initializing the policy and value function, and writing functions for policy evaluation and improvement. Combine these functions in an iterative process to converge to the optimal policy and value function.</p>
                
                        <p>5. How can you visualize the results of policy iteration?</p>
                        <p><strong>Answer:</strong> Use visualization tools, such as Matplotlib, to plot the optimal policy and value function across states. This helps in understanding and analyzing the learned policy and value function.</p>
                    </div>
                    
                    <!-- Embedded YouTube Video -->
                    <div id="videoBlock5" class="video-section">
                        <h3>How to Implement Iterative Policy Evaluation and Update in Python</h3>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/LKPcm3Bh5nI" 
                                title="YouTube video player" frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                allowfullscreen></iframe>
                    </div>
                </div>
            </li>
            
            <li class="card">
                <h2 class="card-title">6. Chatbot Using Bi-directional LSTMs</h2>
                <button class="reveal-button" onclick="toggleContent('theoryContent6')">Toggle Content</button>
                <div id="theoryContent6" class="theoretical-content">
                    <h3>Overview of Bi-directional LSTMs for Chatbots</h3>
                    <p>Bi-directional Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) that process sequences in both forward and backward directions. This bidirectional approach allows the network to understand context from both past and future words, making it highly effective for natural language processing tasks like chatbots.</p>
                    
                    <h3>Key Concepts</h3>
                    <h4>1. Understanding Bi-directional LSTMs</h4>
                    <p><strong>Definition:</strong> A bi-directional LSTM consists of two LSTM layers: one processing the input sequence from start to end and the other processing it from end to start. This allows the model to capture context from both directions.</p>
                    <p><strong>Example:</strong> Implement a bi-directional LSTM in PyTorch using the <code>nn.LSTM</code> module with the <code>bidirectional=True</code> parameter.</p>
                    
                    <h4>2. Building the Chatbot Model</h4>
                    <p><strong>Definition:</strong> The chatbot model uses bi-directional LSTMs to encode user inputs and generate responses. It involves tokenizing the text, encoding it with LSTMs, and then generating a response based on the encoded features.</p>
                    <p><strong>Example:</strong> Build a chatbot model using PyTorch as follows:
                    <pre><code>
import torch
import torch.nn as nn
import torch.optim as optim
            
class BiLSTMChatbot(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, output_size):
        super(BiLSTMChatbot, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(hidden_size * 2, output_size)
            
    def forward(self, x):
        x = self.embedding(x)
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])
        return output
            
# Define parameters
vocab_size = 10000  # Size of vocabulary
embed_size = 100    # Size of embedding vectors
hidden_size = 128   # Size of hidden layers
output_size = 10    # Number of response classes
            
# Initialize model, loss function, and optimizer
model = BiLSTMChatbot(vocab_size, embed_size, hidden_size, output_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
            
# Example training loop
for epoch in range(10):
    # Dummy input and target for demonstration
    input_seq = torch.randint(0, vocab_size, (32, 10))  # Batch size 32, sequence length 10
    target = torch.randint(0, output_size, (32,))        # Batch size 32
            
    optimizer.zero_grad()
    output = model(input_seq)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
            
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')
                    </code></pre>
                    </p>
                    
                    <h4>3. Training the Chatbot</h4>
                    <p><strong>Definition:</strong> Train the chatbot model using a dataset of dialogues. The training involves feeding the model with sequences of text and adjusting the weights based on the loss function to improve response accuracy.</p>
                    <p><strong>Example:</strong> Use a dataset like the Cornell Movie Dialogues corpus to train the chatbot and evaluate its responses.</p>
                    
                    <h4>4. Testing and Fine-tuning</h4>
                    <p><strong>Definition:</strong> After training, test the chatbot with new inputs to evaluate its performance. Fine-tune the model parameters and training data to improve the quality of responses.</p>
                    <p><strong>Example:</strong> Test the chatbot with user queries and adjust hyperparameters or model architecture based on performance metrics.</p>
                    
                    <h3>Practical Use of Bi-directional LSTMs for Chatbots</h3>
                    <h4>Setting Up the Environment</h4>
                    <p>Install PyTorch and other necessary libraries. Prepare the dataset and preprocess text data for training.</p>
                    
                    <h4>Implementing and Training the Chatbot</h4>
                    <p>Build the bi-directional LSTM model, train it with dialogue data, and adjust parameters to improve performance.</p>
                    
                    <h4>Testing and Evaluating Performance</h4>
                    <p>Test the chatbot with various inputs, evaluate its responses, and refine the model to enhance conversational quality.</p>
                    
                    <div id="vivaQuestions6" class="viva-questions">
                        <h3>Viva Questions and Answers</h3>
                        <p>1. What are bi-directional LSTMs and why are they useful for chatbots?</p>
                        <p><strong>Answer:</strong> Bi-directional LSTMs are a type of Recurrent Neural Network that process sequences in both forward and backward directions. They are useful for chatbots as they capture context from both past and future words, improving the understanding and generation of natural language responses.</p>
                
                        <p>2. How do bi-directional LSTMs differ from regular LSTMs?</p>
                        <p><strong>Answer:</strong> Regular LSTMs process sequences in one direction (forward), while bi-directional LSTMs process sequences in both forward and backward directions. This allows bi-directional LSTMs to capture more contextual information from the entire sequence.</p>
                
                        <p>3. What is the role of the embedding layer in the chatbot model?</p>
                        <p><strong>Answer:</strong> The embedding layer converts input tokens into dense vectors of fixed size. This representation captures semantic meaning and relationships between words, which helps the LSTM layers to better understand and process the input sequences.</p>
                
                        <p>4. How do you train a chatbot using bi-directional LSTMs?</p>
                        <p><strong>Answer:</strong> Train the chatbot by feeding it sequences of text and adjusting the model parameters based on the loss function. Use dialogue datasets to train the model and evaluate its performance with test inputs.</p>
                
                        <p>5. How can you evaluate and improve the performance of a chatbot?</p>
                        <p><strong>Answer:</strong> Evaluate the chatbot by testing it with various inputs and analyzing the quality of its responses. Improve performance by fine-tuning the model, adjusting hyperparameters, and using diverse training data.</p>
                    </div>
                    
                    <!-- Embedded YouTube Video -->
                    <div id="videoBlock6" class="video-section">
                        <h3>How to Implement a Chatbot Using Bi-directional LSTMs</h3>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/Xf93T0-K4kQ" 
                                title="YouTube video player" frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                allowfullscreen></iframe>
                    </div>
                </div>
            </li>
            
            

            <li class="card">
                <h2 class="card-title">7. Image Classification on MNIST Dataset (CNN Model with Fully Connected Layer)</h2>
                <button class="reveal-button" onclick="toggleContent('theoryContent7')">Toggle Content</button>
                <div id="theoryContent7" class="theoretical-content">
                    <h3>Overview of CNN for MNIST Classification</h3>
                    <p>Convolutional Neural Networks (CNNs) are specialized neural networks designed for processing structured grid data, such as images. For image classification tasks like the MNIST dataset, CNNs can effectively learn spatial hierarchies and patterns in images. This example demonstrates using a CNN model with fully connected layers to classify handwritten digits from the MNIST dataset.</p>
                    
                    <h3>Key Concepts</h3>
                    <h4>1. Understanding CNNs</h4>
                    <p><strong>Definition:</strong> CNNs use convolutional layers to extract features from images, pooling layers to reduce dimensionality, and fully connected layers to classify the features. Convolutional layers apply filters to detect patterns, while pooling layers downsample the feature maps.</p>
                    <p><strong>Example:</strong> Implement a simple CNN in PyTorch for the MNIST dataset with convolutional, pooling, and fully connected layers.</p>
                    
                    <h4>2. Building the CNN Model</h4>
                    <p><strong>Definition:</strong> The CNN model for MNIST consists of convolutional layers to extract features from images, followed by fully connected layers to make the final classification.</p>
                    <p><strong>Example:</strong> Build a CNN model using PyTorch as follows:
                    <pre><code>
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
            
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)  # 10 output classes for MNIST
            
    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 7 * 7)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x
            
# Load MNIST data
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
            
# Initialize model, loss function, and optimizer
model = CNNModel()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
            
# Training loop
for epoch in range(10):
    running_loss = 0.0
    for images, labels in trainloader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
            
    print(f'Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}')
                    </code></pre>
                    </p>
                    
                    <h4>3. Training the CNN Model</h4>
                    <p><strong>Definition:</strong> Train the CNN model using the MNIST dataset by feeding the images and labels through the network. The model adjusts its weights based on the loss function to improve classification accuracy.</p>
                    <p><strong>Example:</strong> Use the MNIST training set to train the model and evaluate its performance on the test set.</p>
                    
                    <h4>4. Evaluating and Fine-tuning</h4>
                    <p><strong>Definition:</strong> After training, evaluate the model’s performance on the test set. Fine-tune the model parameters and architecture if necessary to improve accuracy.</p>
                    <p><strong>Example:</strong> Test the model on the MNIST test set and adjust hyperparameters or network layers based on accuracy metrics.</p>
                    
                    <h3>Practical Use of CNN for MNIST Classification</h3>
                    <h4>Setting Up the Environment</h4>
                    <p>Install PyTorch and torchvision libraries. Download and preprocess the MNIST dataset for training and testing.</p>
                    
                    <h4>Implementing and Training the CNN Model</h4>
                    <p>Build the CNN model, train it on the MNIST dataset, and adjust parameters to improve performance.</p>
                    
                    <h4>Evaluating and Improving Performance</h4>
                    <p>Evaluate the CNN model on test data, analyze the results, and refine the model as needed to enhance classification accuracy.</p>
                    
                    <div id="vivaQuestions7" class="viva-questions">
                        <h3>Viva Questions and Answers</h3>
                        <p>1. What is a Convolutional Neural Network (CNN) and why is it used for image classification?</p>
                        <p><strong>Answer:</strong> A CNN is a type of neural network specifically designed to process and classify image data. It uses convolutional layers to extract features from images, pooling layers to reduce dimensionality, and fully connected layers to classify the features. CNNs are effective for image classification due to their ability to capture spatial hierarchies and patterns.</p>
                
                        <p>2. How does a convolutional layer work in a CNN?</p>
                        <p><strong>Answer:</strong> A convolutional layer applies a set of filters (kernels) to the input image to detect features such as edges, textures, and patterns. Each filter convolves over the image, producing feature maps that represent the presence of specific features in different parts of the image.</p>
                
                        <p>3. What is the purpose of pooling layers in a CNN?</p>
                        <p><strong>Answer:</strong> Pooling layers reduce the spatial dimensions of feature maps, which decreases the computational complexity and helps to make the model more robust to variations in the input data. Common pooling operations include max pooling and average pooling.</p>
                
                        <p>4. How do you train a CNN model on the MNIST dataset?</p>
                        <p><strong>Answer:</strong> Train the CNN model by feeding it images and labels from the MNIST dataset. Use a loss function such as Cross-Entropy Loss and an optimizer like Adam to adjust the model’s weights based on the loss. Perform multiple epochs of training to improve the model’s performance.</p>
                
                        <p>5. How can you evaluate and improve the performance of a CNN model?</p>
                        <p><strong>Answer:</strong> Evaluate the CNN model by testing it on a separate test set and analyzing its accuracy. To improve performance, consider fine-tuning hyperparameters, adjusting network architecture, or using data augmentation techniques to enhance model generalization.</p>
                    </div>
                    
                    <!-- Embedded YouTube Video -->
                    <div id="videoBlock7" class="video-section">
                        <h3>How to Implement Image Classification on MNIST Dataset Using CNN</h3>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/JVCCXT3sAgw" 
                                title="YouTube video player" frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                allowfullscreen></iframe>
                    </div>
                </div>
            </li>            

            <li class="card">
                <h2 class="card-title">8. Train a Sentiment Analysis Model on IMDB Dataset (RNN Layers with LSTM/GRU)</h2>
                <button class="reveal-button" onclick="toggleContent('theoryContent8')">Toggle Content</button>
                <div id="theoryContent8" class="theoretical-content">
                    <h3>Overview of RNN for Sentiment Analysis</h3>
                    <p>Recurrent Neural Networks (RNNs) are designed for sequence data and can capture temporal dependencies. For sentiment analysis tasks like those on the IMDB dataset, RNNs with Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU) can effectively model the sequence of words in a review to determine sentiment. This example demonstrates how to train a sentiment analysis model using RNN layers with LSTM/GRU on the IMDB dataset.</p>
                    
                    <h3>Key Concepts</h3>
                    <h4>1. Understanding RNNs with LSTM/GRU</h4>
                    <p><strong>Definition:</strong> RNNs process sequences of data by maintaining a hidden state that evolves over time. LSTMs and GRUs are advanced RNN architectures that address the vanishing gradient problem by using gating mechanisms to control the flow of information.</p>
                    <p><strong>Example:</strong> Implement an RNN model with LSTM or GRU layers in PyTorch for sentiment analysis.</p>
                    
                    <h4>2. Building the Sentiment Analysis Model</h4>
                    <p><strong>Definition:</strong> The sentiment analysis model uses RNN layers with LSTM or GRU to process and encode text sequences, followed by fully connected layers to classify the sentiment of the input text.</p>
                    <p><strong>Example:</strong> Build a sentiment analysis model using PyTorch as follows:
                    <pre><code>
import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.datasets import IMDB
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import DataLoader
from torchtext.data.utils import to_map_style_dataset
            
# Define the model
class SentimentRNN(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, output_size, rnn_type='LSTM'):
        super(SentimentRNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        if rnn_type == 'LSTM':
            self.rnn = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)
        elif rnn_type == 'GRU':
            self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, output_size)  # 2 for bidirectional
            
    def forward(self, x):
        x = self.embedding(x)
        rnn_out, _ = self.rnn(x)
        x = rnn_out[:, -1, :]  # Use output from last time step
        x = self.fc(x)
        return x
            
# Load and preprocess IMDB data
def yield_tokens(data_iter):
    for _, text in data_iter:
        yield tokenizer(text)
            
tokenizer = get_tokenizer('basic_english')
train_iter = IMDB(split='train')
vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>'])
vocab.set_default_index(vocab['<unk>'])
            
def process_data(data_iter):
    for label, text in data_iter:
        yield torch.tensor(vocab(tokenizer(text)), dtype=torch.long), 0 if label == 'neg' else 1
            
train_data = to_map_style_dataset(process_data(IMDB(split='train')))
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
            
# Define parameters
vocab_size = len(vocab)
embed_size = 100
hidden_size = 128
output_size = 2  # Binary classification
model = SentimentRNN(vocab_size, embed_size, hidden_size, output_size, rnn_type='LSTM')  # or 'GRU'
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
            
# Training loop
for epoch in range(10):
    running_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
            
    print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}')
                    </code></pre>
                    </p>
                    
                    <h4>3. Training the Sentiment Analysis Model</h4>
                    <p><strong>Definition:</strong> Train the sentiment analysis model using the IMDB dataset by feeding the text sequences and corresponding sentiment labels through the network. The model adjusts its weights based on the loss function to improve sentiment classification accuracy.</p>
                    <p><strong>Example:</strong> Use the IMDB training set to train the model and evaluate its performance on a validation set.</p>
                    
                    <h4>4. Evaluating and Fine-tuning</h4>
                    <p><strong>Definition:</strong> After training, evaluate the model’s performance on a separate test set. Fine-tune the model parameters and architecture if necessary to enhance accuracy.</p>
                    <p><strong>Example:</strong> Test the model on the IMDB test set and adjust hyperparameters or network architecture based on accuracy and loss metrics.</p>
                    
                    <h3>Practical Use of RNN for Sentiment Analysis</h3>
                    <h4>Setting Up the Environment</h4>
                    <p>Install PyTorch and torchtext libraries. Download and preprocess the IMDB dataset for training and testing.</p>
                    
                    <h4>Implementing and Training the Sentiment Analysis Model</h4>
                    <p>Build the RNN model with LSTM or GRU layers, train it on the IMDB dataset, and adjust parameters to improve performance.</p>
                    
                    <h4>Evaluating and Improving Performance</h4>
                    <p>Evaluate the RNN model on test data, analyze the results, and refine the model as needed to enhance classification accuracy.</p>
                    
                    <div id="vivaQuestions8" class="viva-questions">
                        <h3>Viva Questions and Answers</h3>
                        <p>1. What is the role of RNNs in sentiment analysis?</p>
                        <p><strong>Answer:</strong> RNNs are designed to handle sequence data and can capture temporal dependencies in text. In sentiment analysis, RNNs process sequences of words to understand the context and sentiment of a review, which helps in classifying the sentiment as positive or negative.</p>
                
                        <p>2. How do LSTM and GRU improve upon traditional RNNs?</p>
                        <p><strong>Answer:</strong> LSTM and GRU are advanced RNN architectures that address the vanishing gradient problem. They use gating mechanisms to control the flow of information, allowing the network to learn long-range dependencies more effectively than traditional RNNs.</p>
                
                        <p>3. What are the key components of an LSTM/GRU layer?</p>
                        <p><strong>Answer:</strong> Key components of LSTM layers include the input gate, forget gate, and output gate, which control the flow of information and manage the cell state. GRU layers use update and reset gates to achieve similar functionality with a simpler architecture.</p>
                
                        <p>4. How do you train an RNN model for sentiment analysis on the IMDB dataset?</p>
                        <p><strong>Answer:</strong> Train the RNN model by feeding it text sequences and corresponding sentiment labels from the IMDB dataset. Use a loss function such as Cross-Entropy Loss and an optimizer like Adam to adjust the model’s weights. Perform multiple epochs of training to improve model accuracy.</p>
                
                        <p>5. How can you evaluate and improve the performance of an RNN model?</p>
                        <p><strong>Answer:</strong> Evaluate the RNN model by testing it on a separate test set and analyzing metrics such as accuracy and loss. To improve performance, consider adjusting hyperparameters, modifying the network architecture, or using more diverse training data.</p>
                    </div>
                    
                    <!-- Embedded YouTube Video -->
                    <div id="videoBlock8" class="video-section">
                        <h3>How to Implement Sentiment Analysis on IMDB Dataset Using RNN with LSTM/GRU</h3>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/9os1Tcb-Y3Y" 
                                title="YouTube video player" frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                allowfullscreen></iframe>
                    </div>
                </div>
            </li>
            
            <li class="card">
                <h2 class="card-title">9. Applying Deep Learning Models in the Field of Natural Language Processing (NLP)</h2>
                <button class="reveal-button" onclick="toggleContent('theoryContent9')">Toggle Content</button>
                <div id="theoryContent9" class="theoretical-content">
                    <h3>Overview of Deep Learning in NLP</h3>
                    <p>Deep learning has revolutionized Natural Language Processing (NLP) by enabling models to understand and generate human language with high accuracy. Techniques such as Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and Transformer models are extensively used to handle various NLP tasks including text classification, sentiment analysis, machine translation, and language generation.</p>
                    
                    <h3>Key Concepts</h3>
                    <h4>1. Understanding Deep Learning Models for NLP</h4>
                    <p><strong>Definition:</strong> Deep learning models in NLP utilize neural networks to process and analyze text data. Models like RNNs, LSTMs, GRUs, and Transformers capture complex patterns and dependencies in language data to perform tasks such as text classification, translation, and summarization.</p>
                    <p><strong>Example:</strong> Implement a text classification model using Transformer architecture like BERT in PyTorch.</p>
                    
                    <h4>2. Building Deep Learning Models for NLP</h4>
                    <p><strong>Definition:</strong> Deep learning models for NLP can be built using architectures such as RNNs, LSTMs, GRUs, and Transformers. These models are trained on large text corpora to learn representations of language and perform specific NLP tasks.</p>
                    <p><strong>Example:</strong> Build a text classification model using the BERT Transformer model with PyTorch as follows:
                    <pre><code>
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, Dataset
import torch
            
# Define a custom dataset
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
            
    def __len__(self):
        return len(self.texts)
            
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }
            
# Initialize tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
            
# Example data
texts = ["I love this movie!", "I hate this movie."]
labels = [1, 0]  # 1 for positive, 0 for negative
dataset = TextDataset(texts, labels, tokenizer, max_len=32)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
            
# Define optimizer
optimizer = AdamW(model.parameters(), lr=1e-5)
            
# Training loop
model.train()
for epoch in range(3):
    total_loss = 0
    for batch in dataloader:
        optimizer.zero_grad()
        outputs = model(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            labels=batch['label']
        )
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
            
    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')
                    </code></pre>
                    </p>
                    
                    <h4>3. Training Deep Learning Models for NLP</h4>
                    <p><strong>Definition:</strong> Train deep learning models by feeding them text data and corresponding labels. The models learn to capture language patterns and perform tasks such as classification or translation by optimizing their parameters based on a loss function.</p>
                    <p><strong>Example:</strong> Train a BERT model for text classification using a labeled dataset and evaluate its performance on a validation set.</p>
                    
                    <h4>4. Evaluating and Fine-tuning</h4>
                    <p><strong>Definition:</strong> After training, evaluate the model’s performance on a test set and fine-tune the model if necessary to improve accuracy. This may involve adjusting hyperparameters, changing the model architecture, or using more training data.</p>
                    <p><strong>Example:</strong> Test the model on a separate test set, analyze performance metrics, and adjust model parameters to optimize accuracy and performance.</p>
                    
                    <h3>Practical Use of Deep Learning Models in NLP</h3>
                    <h4>Setting Up the Environment</h4>
                    <p>Install necessary libraries such as PyTorch and Hugging Face's transformers. Download and preprocess text data for training and evaluation.</p>
                    
                    <h4>Implementing and Training Deep Learning Models</h4>
                    <p>Build and train deep learning models like Transformers for various NLP tasks. Adjust parameters and architecture to improve performance.</p>
                    
                    <h4>Evaluating and Improving Performance</h4>
                    <p>Evaluate models on test data, analyze results, and refine the models as needed to enhance accuracy and effectiveness in NLP tasks.</p>
                    
                    <div id="vivaQuestions9" class="viva-questions">
                        <h3>Viva Questions and Answers</h3>
                        <p>1. How do deep learning models contribute to NLP tasks?</p>
                        <p><strong>Answer:</strong> Deep learning models, such as RNNs, LSTMs, GRUs, and Transformers, enhance NLP tasks by capturing complex language patterns and dependencies. They enable accurate text classification, sentiment analysis, translation, and other language-related tasks by learning from large amounts of text data.</p>
                
                        <p>2. What is the advantage of using Transformers like BERT in NLP?</p>
                        <p><strong>Answer:</strong> Transformers, such as BERT, are designed to handle long-range dependencies and contextual information effectively. They use attention mechanisms to weigh the importance of different words in a sentence, improving the understanding and generation of text compared to traditional RNNs and LSTMs.</p>
                
                        <p>3. What are the key components of a Transformer model?</p>
                        <p><strong>Answer:</strong> Key components of a Transformer model include the attention mechanism, encoder layers, and decoder layers. The attention mechanism allows the model to focus on different parts of the input sequence, while the encoder and decoder layers process and generate text sequences.</p>
                
                        <p>4. How do you train a deep learning model for an NLP task?</p>
                        <p><strong>Answer:</strong> Train a deep learning model by feeding it text data and corresponding labels. Use a suitable loss function and optimizer to adjust the model's weights during training. Perform multiple epochs and evaluate the model's performance on a validation set to ensure accurate results.</p>
                
                        <p>5. How can you evaluate and improve the performance of an NLP model?</p>
                        <p><strong>Answer:</strong> Evaluate the NLP model using metrics such as accuracy, precision, recall, and F1 score on a test set. To improve performance, consider fine-tuning hyperparameters, modifying the model architecture, using more data, or employing data augmentation techniques.</p>
                    </div>
                    
                    <!-- Embedded YouTube Video -->
                    <div id="videoBlock9" class="video-section">
                        <h3>How to Apply Deep Learning Models in NLP</h3>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/B4N3jnbgQ5I" 
                                title="YouTube video player" frameborder="0" 
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                allowfullscreen></iframe>
                    </div>
                </div>
            </li>            

                


        </ul>
    </section>

    <script>
        function toggleContent(contentId) {
            var content = document.getElementById(contentId);

            // Check if content is hidden (display is "none" or initial empty value)
            if (content.style.display === "none" || content.style.display === "") {
                content.style.display = "block"; // Show content
            } else {
                content.style.display = "none"; // Hide content
            }
        }
    </script>
</body>

</html>
